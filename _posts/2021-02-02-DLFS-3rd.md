---
layout: post
<<<<<<< HEAD
title: "밑바닥부터 시작하는 딥러닝 3"
=======
title: "밑바닥부터 시작하는 딥러닝 2"
>>>>>>> d4c6562f41c68357b3bcde1e850398da1c4adec4
date: 2021-02-02 22:53:55 +0900
author: JunYoung
categories: CS ML
tags: Machine_Learning Deep_Learning Python
math: true
---

# Ch5

## 역전파

Ch4에서 사용한 수치미분방법은 딥러닝에서 손실함수의 gradient를 계산하기에는 너무 오래 걸린다.

역전파로 기울기를 계산하는 것은, 일종의 dynamic programming 인 데다가, 모델의 손실 함수를
이미 미분 식이 알려진 함수들의 합성으로 표현하여, chain rule로 전체 gradient를 표기한다.
그렇게 하면 많은 계산의 중복을 줄일 수 있다.

사실 차이가 그렇게까지 크진 않을 줄 알았는데, 막상 테스트해보니 차이가 엄청 컸다.

Time complexity의 분석을 해 보자.

우선 편의상 데이터는 $$n$$차원 데이터 $$1$$개가 주어졌다고 하자.(나중에 batch의 크기를 곱해 주면 되니까)

$$x \in \mathbb{R}^n$$이 주어진 데이터라고 하자, $$ \sigma_1(x) $$를 sigmoid 함수,
$$\sigma_2(x)$$를 softmax함수라 하자.

$$
y = \sigma_{2} (\sigma_{1}(x \cdot W_1 + b_1) \cdot W_2 + b_2)
$$

라 할 수 있다.

$$
\left (W_1 \in Mat_{n \times m} , \; b_1 \in \mathbb{R}^m ,\; W_2 \in Mat_{m \times t}, \;b_2 \in \mathbb{R}^t\right)
$$

그래프에서 출력이 전파되느니 뭐니 하니까 뭔소린지 처음에 헷갈리는데,
잘 생각해 보면 그냥

$$
(f(g(h(x))))' = f'(g(h(x)))g'(h(x))h'(x)
$$

에서 $$g(h(x)),\; h(x)$$ 의 값은 여러 번 사용되니까 저장해 두고(= dynamic programming),
$$f(x), g(x), h(x)$$각각의 미분은 잘 알려져 있으므로 주어진 수식을 잘 파싱하여
체인 룰을 적용할 수 있게 만드는 것이다.

일반적으로 수식이나 코드 파싱할 때 그래프(보통 트리)를 써서 표현하는 걸 생각하면 자연스럽다고 할 수 있다.

특히 딥러닝 모델에서는 parameter의 크기가 많아서 미분할 변수가 많으므로 중복을 줄이는 게 큰 차이가 난다.
자세한 분석은 밑 문단을 참고하자.

## 수행 시간의 분석

### 수치 미분의 시간복잡도

기존의 수치 미분 알고리즘에서는,
$$ W_1, W_2, b_1 , b_2$$ 를 가지고 미분을 하므로, 벡터에 대하여 gradient를 계산하는 `numerical_gradient()`함수를
총 $$ n + 1 + m + 1 = n + m + 2 $$번 호출한다.

그리고, 각 `numerical_gradient()`함수에서는, 벡터의 각 성분에 대한
손실 함수의 차이를 계산하여야 하므로 `loss_func()`를
$$ m + m + t + t = 2(m + t)$$ 번 호출한다.

`loss_func()`함수는, 교차 엔트로피를 계산하므로, 모델의 `predict()`함수를 한 번 실행한다.
또한 결과 벡터의 각 원소에 로그를 씌우고, 정답 벡터와 곱해서 더해야 하므로 여기에 $$O(n)$$의 연산이 필요하다.

`predict()`함수에서는, 행렬곱 두 번과, 행렬의 합 두 번, 그리고 sigmoid와 softmax함수의 연산이 필요하다.
($$y = \sigma_{2} (\sigma_{1}(x \cdot W_1 + b_1) \cdot W_2 + b_2)$$를 실제로 계산하므로)

sigmoid와 softmax함수는 각 벡터의 개수에 선형적으로 비례할 것이고, 행렬의 합 또한 그러하다.
행렬곱은 물론 `numpy`의 계산 알고리즘에 따라 다르겠지만, 어차피 역전파와의 비교가 목적이므로,
일단 $$ O(nm + mt)$$라 하자.

이상의 계산 과정을 거치면, 총 $$O((n + m)(m + t)(nm + mt))$$의 시간복잡도가 도출된다.

### 역전파의 시간복잡도

가장 시간복잡도가 큰

$$
\displaystyle \frac{dy}{dW_1} = \frac{d \sigma_2}{du} \times \frac{du}{d\sigma_{1}} \times \frac{d\sigma_1}{dv} \times \frac{dv}{dW_1}
$$

만 따지면 된다.

- $$\displaystyle \frac{d \sigma_2}{du}$$ 는 총 $$ O(t) $$의 시간복잡도가 든다.
- $$\displaystyle \frac{du}{d\sigma_{1}}$$ 는 총 $$ O(mt) $$의 시간복잡도가 든다.
- $$\displaystyle \frac{d\sigma_1}{dv}$$ 는 총 $$ O(m) $$의 시간복잡도가 든다.
- $$\displaystyle \frac{dv}{dW_1}$$ 는 총 $$ O(nm) $$의 시간복잡도가 든다.

즉 총 $$O(nm + mt)$$의 시간복잡도가 든다.

(위의 표기에서

$$
\displaystyle  u = \sigma_1 (x \cdot W_1 + b_1) \cdot W_2 + b_2 ,\; v = x \cdot W_1 + b_1
$$

이다. 즉

$$
\displaystyle  y = \sigma_2(u)= \sigma_2(\sigma_1(v) \cdot W_2 + b_2) =\sigma_2(\sigma_1(x \cdot W_1 + b_1) \cdot W_2 + b_2)
$$

이다.)

### 결론

따라서 두 과정을 비교해 보면, 역전파의 시간복잡도는 수치 미분에서의 엄청나게 많은 중복을 dynamic programming과
적절한 symbolic differentiation으로 인하여 상수배로 줄였다.

또한 계산 과정을 생각해보면, 시간복잡도에서 상수배 계산과, 다른 항에 지배되는 항들은 생략되지만, 실제로는 그 항들의 크기도
꽤 클 것으로 생각된다. (`loss_func`함수에서의 교차 엔트로피 계산 등, 특히 Softmax와 교차 엔트로피 손실 함수를 같이 쓰면 역전파에서 미분이 매우 간단하게 나온다.)

지금의 예시에서도 충분히 큰 차이가 나지만, 모델의 층이 깊어져서 계산그래프의 깊이도 깊어진다고 가정하자.
그 깊이를 $$l$$이라 하면, 수치 미분의 경우 매 층마다 중복되는 계산이 입출력 parameter의 개수만큼 생기므로
시간복잡도가 지수적으로 더 늘어날 것이다.
역전파의 시간복잡도는 Worst Case에 $$O(l^2)$$이 곱해질 것이고,
일반적으로는 $$O(l)$$이 곱해질 것이다.
(<a href ="https://www.deeplearningbook.org/">\<심층 학습\>(이안 굿펠로, 요수아 벤지오, 에런 쿠빌)</a> 6장 참고)

## 여담

데이터 사이언스 책과 딥러닝 책 같이 공부하다 보니 알았는데,
데이터 사이언스 책 7장에서 수량형 x - 범주형 y 변수를 회귀분석 할 때
$$ \displaystyle \textrm{logit}(\mu) = \log \left( \frac{ \mu} {1 - \mu}\right) = x \beta$$ 라 놓고,
$$ \displaystyle \textrm{logistic}(\eta) = \frac{1}{1 + \exp(- \eta) }$$로 계산하는데,
이 logistic함수가 sigmoid함수이고, 신경망의 1층과 같은 형태이다.
이걸 왜 분류 문제에 쓰는지도 더 직관적으로 이해가 잘 된다.
수리통계랑 최적화 자세히 배우면 뭔가 이것저것 재밌는걸 많이 할 수 있을 것 같은데, 언제 듣지...
